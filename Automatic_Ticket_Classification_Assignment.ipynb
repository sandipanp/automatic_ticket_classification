{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhR-ZUkwJrFn"
   },
   "source": [
    "## Problem Statement \n",
    "\n",
    "You need to build a model that is able to classify customer complaints based on the products/services. By doing so, you can segregate these tickets into their relevant categories and, therefore, help in the quick resolution of the issue.\n",
    "\n",
    "You will be doing topic modelling on the <b>.json</b> data provided by the company. Since this data is not labelled, you need to apply NMF to analyse patterns and classify tickets into the following five clusters based on their products/services:\n",
    "\n",
    "* Credit card / Prepaid card\n",
    "\n",
    "* Bank account services\n",
    "\n",
    "* Theft/Dispute reporting\n",
    "\n",
    "* Mortgages/loans\n",
    "\n",
    "* Others \n",
    "\n",
    "\n",
    "With the help of topic modelling, you will be able to map each ticket onto its respective department/category. You can then use this data to train any supervised model such as logistic regression, decision tree or random forest. Using this trained model, you can classify any new customer complaint support ticket into its relevant department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcgXVNyaLUFS"
   },
   "source": [
    "## Pipelines that needs to be performed:\n",
    "\n",
    "You need to perform the following eight major tasks to complete the assignment:\n",
    "\n",
    "1.  Data loading\n",
    "\n",
    "2. Text preprocessing\n",
    "\n",
    "3. Exploratory data analysis (EDA)\n",
    "\n",
    "4. Feature extraction\n",
    "\n",
    "5. Topic modelling \n",
    "\n",
    "6. Model building using supervised learning\n",
    "\n",
    "7. Model training and evaluation\n",
    "\n",
    "8. Model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuLFIymAL58u"
   },
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-Q9pqrcJrFr"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, string\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtRLCsNVJrFt"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "The data is in JSON format and we need to convert it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puVzIf_iJrFt"
   },
   "outputs": [],
   "source": [
    "# Opening JSON file \n",
    "f = f = open(\"data/complaints-2021-05-14_08_16.json\")# Write the path to your data file and load it \n",
    "  \n",
    "# returns JSON object as  \n",
    "# a dictionary \n",
    "data = json.load(f)\n",
    "df=pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xYpH-sAJrFu"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "Lf8ufHH5JrFu",
    "outputId": "329b0aac-6f89-4af6-a8ee-40083c173eca"
   },
   "outputs": [],
   "source": [
    "# Inspect the dataframe to understand the given data.\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFTbwXMaMNWI",
    "outputId": "9e380fe4-82ad-4b85-b290-6808f5c248d2"
   },
   "outputs": [],
   "source": [
    "# Inspect the dataset information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dwcty-wmJrFw",
    "outputId": "9d3b0766-1033-47d6-e137-73a1cf32efa2"
   },
   "outputs": [],
   "source": [
    "#print the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLlAL3zDMiGR",
    "outputId": "9347e91a-1e95-499a-cd4b-17733abcf3c2"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYCtKXD1JrFw",
    "outputId": "30ee9423-c9f4-41ed-dccd-1cb4cf1e653b"
   },
   "outputs": [],
   "source": [
    "#Assign new column names\n",
    "#Removing _source from column name\n",
    "df_columns = df.columns\n",
    "new_column_names = [ column.replace(\"_source.\",\"\") for column in df_columns]\n",
    "df.columns = new_column_names\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZsOHBWXM5PP",
    "outputId": "a1ee85b2-d0b2-4018-df4d-f005a02b0e96"
   },
   "outputs": [],
   "source": [
    "#Checking the null values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYmQttKnNG8n",
    "outputId": "19053d18-1988-432a-f5c5-dcc00880ad0a"
   },
   "outputs": [],
   "source": [
    "print(\"Number of empty string in complaint_what_happend :\",(df[\"complaint_what_happened\"]=='').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grQUPFL5JrFx",
    "outputId": "bc7f4d3e-eb44-4b1f-dc7d-61570bcd4753"
   },
   "outputs": [],
   "source": [
    "#Assign nan in place of blanks in the complaints column\n",
    "df[\"complaint_what_happened\"] = df[\"complaint_what_happened\"].apply(lambda x : pd.NA if len(x)==0 else x)\n",
    "print(\"Number of empty string in complaint :\",(df[\"complaint_what_happened\"]=='').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNZZwrMuOEJD",
    "outputId": "c918ae95-7912-4423-84ad-954a1c7d502d"
   },
   "outputs": [],
   "source": [
    "# Checking nan values in complaint column\n",
    "print(\"Now nan values in complain column : \",df[\"complaint_what_happened\"].isnull().sum())\n",
    "df[\"complaint_what_happened\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jfxd8VSmJrFy",
    "outputId": "c6dd9ca4-65d5-4dec-9cde-a52ca4cb4b4a"
   },
   "outputs": [],
   "source": [
    "#Remove all rows where complaints column is nan\n",
    "df=df[~df[\"complaint_what_happened\"].isna()]\n",
    "print(\"Now nan values in complain column : \",df[\"complaint_what_happened\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzKwJlCMORUx",
    "outputId": "018608a3-0d17-49ee-9db1-1d4ae97dbb3b"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L944HZpsJrFy"
   },
   "source": [
    "## Prepare the text for topic modeling\n",
    "\n",
    "Once you have removed all the blank complaints, you need to:\n",
    "\n",
    "* Make the text lowercase\n",
    "* Remove text in square brackets\n",
    "* Remove punctuation\n",
    "* Remove words containing numbers\n",
    "\n",
    "\n",
    "Once you have done these cleaning operations you need to perform the following:\n",
    "* Lemmatize the texts\n",
    "* Extract the POS tags of the lemmatized text and remove all the words which have tags other than NN[tag == \"NN\"].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qm7SjjSkJrFz"
   },
   "outputs": [],
   "source": [
    "# Write your function here to clean the text and remove all the unnecessary elements.\n",
    "def clean_text(text) :\n",
    "  text = text.lower() # Make the text lowercase\n",
    "  text = re.sub(r'\\[.*?\\]', '', text) # Remove text in square brackets\n",
    "  text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "  text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)  #Remove words containing numbers\n",
    "  return text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NjpwLQsOzAx"
   },
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame(df['complaint_what_happened'].apply(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "TV1ZMCtiO8ex",
    "outputId": "9713bd3c-6467-4aa6-a5bc-27bdf7e813ac"
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgOu8t8HJrFz"
   },
   "outputs": [],
   "source": [
    "#Write your function to Lemmatize the texts\n",
    "def lemmmatize_text(text):\n",
    "    sent = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        sent.append(token.lemma_)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXnN7aa_JrF0"
   },
   "outputs": [],
   "source": [
    "#Create a dataframe('df_clean') that will have only the complaints and the lemmatized complaints \n",
    "df_clean['complaint_lemmatized'] = df_clean['complaint_what_happened'].apply(lemmmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "nOiDVvEIJrF0",
    "outputId": "b5f7f836-ea94-4be6-c436-d960d2d408b6"
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIZezeNOSQOB"
   },
   "outputs": [],
   "source": [
    "# Import Textblob for extracting noun phrases\n",
    "!pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phZ5HuFSTJee",
    "outputId": "4bb75117-9545-4648-9a5e-b6f1ef7545fa"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk7fc4DuJrF1"
   },
   "outputs": [],
   "source": [
    "#Write your function to extract the POS tags \n",
    "\n",
    "def pos_tag(text):\n",
    "  # write your code here\n",
    "  sent = []\n",
    "  blob = TextBlob(text)\n",
    "  sent = [word for (word,tag) in blob.tags if tag=='NN']\n",
    "  return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vv_3qM5TRv7D"
   },
   "outputs": [],
   "source": [
    "df_clean[\"complaint_POS_removed\"] =   df_clean['complaint_lemmatized'].apply(pos_tag) #this column should contain lemmatized text with all the words removed which have tags other than NN[tag == \"NN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "AjxfchvFJrF2",
    "outputId": "80008b3e-dc15-434e-f60a-2906d325c66d"
   },
   "outputs": [],
   "source": [
    "#The clean dataframe should now contain the raw complaint, lemmatized complaint and the complaint after removing POS tags.\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Un1AElJrF2"
   },
   "source": [
    "## Exploratory data analysis to get familiar with the data.\n",
    "\n",
    "Write the code in this task to perform the following:\n",
    "\n",
    "*   Visualise the data according to the 'Complaint' character length\n",
    "*   Using a word cloud find the top 40 words by frequency among all the articles after processing the text\n",
    "*   Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text. ‘\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "q-zaqJF6JrF2",
    "outputId": "e59eb291-cbc0-4ccc-88a2-910b8bb5ba69"
   },
   "outputs": [],
   "source": [
    "# Write your code here to visualise the data according to the 'Complaint' character length\n",
    "\n",
    "character_length = [len(each_sent) for each_sent in df_clean['complaint_POS_removed']]\n",
    "\n",
    "sns.displot(character_length, kind='hist', bins=70)\n",
    "plt.xlabel(\"Complaint character length\")\n",
    "plt.ylabel(\"Number of Complaints\")\n",
    "plt.title(\"Distribution of Complaint character length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9jD_6SeJrF3"
   },
   "source": [
    "#### Find the top 40 words by frequency among all the articles after processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4822KLnYbAE"
   },
   "outputs": [],
   "source": [
    "# Installing wordcloud\n",
    "!pip install wordcloud\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "QcfdvtfZJrF3",
    "outputId": "35134af5-aef1-4752-f1b1-8f0bf2f3138a"
   },
   "outputs": [],
   "source": [
    "#Using a word cloud find the top 40 words by frequency among all the articles after processing the text\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "word_cloud = WordCloud(max_font_size=70, max_words=40, background_color=\"white\", random_state=100, stopwords=stopwords)\n",
    "word_cloud.generate(str(df_clean['complaint_POS_removed']))\n",
    "plt.figure(figsize=[15,15])\n",
    "plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkSmc3UaJrF4"
   },
   "outputs": [],
   "source": [
    "#Removing -PRON- from the text corpus \n",
    "df_clean['Complaint_clean'] = df_clean['complaint_POS_removed'].str.replace('-PRON-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4NcUwZlwujx",
    "outputId": "98c3ce6f-3899-4f4e-a5a6-721765ab1a58"
   },
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "afzkY2awbYIO",
    "outputId": "3924cf39-7fda-4e6c-8ac2-d137b35668d5"
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DfCSbbmJrF4"
   },
   "source": [
    "#### Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVrlhW2yb22G"
   },
   "outputs": [],
   "source": [
    "# Creating a function to extract top ngrams(unigram/bigram/trigram) based on the function inputs\n",
    "def get_ngrams(text, n=None, ngram=(1,1)):\n",
    "  vec = CountVectorizer(stop_words='english', ngram_range=ngram).fit(text)\n",
    "  bagofwords = vec.transform(text)\n",
    "  sum_words = bagofwords.sum(axis=0)\n",
    "  words_frequency = [(word, sum_words[0, index]) for word, index in vec.vocabulary_.items()]\n",
    "  words_frequency = sorted(words_frequency, key = lambda x: x[1], reverse=True)\n",
    "  return words_frequency[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "5mbk5DS5JrF4",
    "outputId": "2d1cf59d-bcf8-4e3d-d184-61744830445a"
   },
   "outputs": [],
   "source": [
    "#Write your code here to find the top 30 unigram frequency among the complaints in the cleaned datafram(df_clean). \n",
    "top_30words = get_ngrams(df_clean['Complaint_clean'].values.astype('U'), n=30, ngram=(1,1))\n",
    "df_unigram = pd.DataFrame(top_30words, columns=['unigram', 'count'])\n",
    "df_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "uM7x-0VdcV8L",
    "outputId": "925d81fd-8be3-4f49-884a-fe55cbb66e47"
   },
   "outputs": [],
   "source": [
    "# Plotting top 30 Unigrams\n",
    "plt.figure(figsize=[25,6])\n",
    "sns.barplot(x=df_unigram['unigram'], y=df_unigram['count'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Unigram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of top 30 Unigrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "YX7fedm1JrF8",
    "outputId": "ea3a6a52-a6cc-42a1-d849-6142fcc5ab2d"
   },
   "outputs": [],
   "source": [
    "#Print the top 10 words in the unigram frequency\n",
    "df_unigram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "id": "WOPkProJchJ5",
    "outputId": "42ad39ff-2a18-4fe9-ae72-34a42772c68e"
   },
   "outputs": [],
   "source": [
    "# Plotting top 10 words\n",
    "plt.figure(figsize=[25,6])\n",
    "sns.barplot(x=df_unigram['unigram'].head(10), y=df_unigram['count'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Unigram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of top 30 Unigrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "aV7kD7w8JrF8",
    "outputId": "3852e506-ab60-4040-e4c3-9b8138c8aafd"
   },
   "outputs": [],
   "source": [
    "#Write your code here to find the top 30 bigram frequency among the complaints in the cleaned datafram(df_clean). \n",
    "top_30words = get_ngrams(df_clean['Complaint_clean'].values.astype('U'), n=30, ngram=(2,2))\n",
    "df_bigram = pd.DataFrame(top_30words, columns=['bigram', 'count'])\n",
    "df_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "DOkpIHP-dDEs",
    "outputId": "5dc87d0e-8da8-40e5-88c7-af3af0d22da8"
   },
   "outputs": [],
   "source": [
    "# Plotting top 30 Bigrams\n",
    "plt.figure(figsize=[25,6])\n",
    "sns.barplot(x=df_bigram['bigram'], y=df_bigram['count'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Bigram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of top 30 Bigrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "NPnMNIpyJrF9",
    "outputId": "685ca7db-5c72-4270-eede-88af7b680dc6"
   },
   "outputs": [],
   "source": [
    "#Print the top 10 words in the bigram frequency\n",
    "df_bigram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "0GTRmUQqdjZL",
    "outputId": "448a454e-a38b-4714-e3be-174513f4b6cb"
   },
   "outputs": [],
   "source": [
    "# Plotting top 10 Bugrams\n",
    "plt.figure(figsize=[25,6])\n",
    "sns.barplot(x=df_bigram['bigram'].head(10), y=df_bigram['count'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Bigram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of top 10 Bigrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "Xkh7vtbtJrF-",
    "outputId": "f51980b0-a671-4bed-b9f2-1e22ffa1acd6"
   },
   "outputs": [],
   "source": [
    "#Write your code here to find the top 30 trigram frequency among the complaints in the cleaned datafram(df_clean). \n",
    "top_30words = get_ngrams(df_clean['Complaint_clean'].values.astype('U'), n=30, ngram=(3,3))\n",
    "df_trigram = pd.DataFrame(top_30words, columns=['trigram', 'count'])\n",
    "df_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "euqFuJntd6yT",
    "outputId": "614cb254-8824-4912-887e-eb1e71ea460f"
   },
   "outputs": [],
   "source": [
    "# Plotting top 30 Trigrams\n",
    "plt.figure(figsize=[25,6])\n",
    "sns.barplot(x=df_trigram['trigram'], y=df_trigram['count'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Trigram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of top 30 Trigrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "REcVxNfvJrF-",
    "outputId": "83a681f2-8ef7-40bb-82f6-5f35396dbb62"
   },
   "outputs": [],
   "source": [
    "#Print the top 10 words in the trigram frequency\n",
    "df_trigram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "YHr4A6abeU_Z",
    "outputId": "129d36d9-abbc-4bf9-d0a9-903aaa7a26c0"
   },
   "outputs": [],
   "source": [
    "# Plotting top 10 Trigrams\n",
    "plt.figure(figsize=[25,6])\n",
    "sns.barplot(x=df_trigram['trigram'].head(10), y=df_trigram['count'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Trigram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of top 10 Trigrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUXzFji0JrF_"
   },
   "source": [
    "## The personal details of customer has been masked in the dataset with xxxx. Let's remove the masked text as this will be of no use for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKda-a_IJrF_"
   },
   "outputs": [],
   "source": [
    "df_clean['Complaint_clean'] = df_clean['Complaint_clean'].str.replace('xxxx','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4Vd4Y6HwlVP",
    "outputId": "f667a16e-03db-40de-c2d6-6bee2b531403"
   },
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "9UIFk8fQJrF_",
    "outputId": "4c5aba8f-1a70-433a-8baf-bb1cc76ca3b5"
   },
   "outputs": [],
   "source": [
    "#All masked texts has been removed\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-I0k0QtJrGA"
   },
   "source": [
    "## Feature Extraction\n",
    "Convert the raw texts to a matrix of TF-IDF features\n",
    "\n",
    "**max_df** is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\"\n",
    "max_df = 0.95 means \"ignore terms that appear in more than 95% of the complaints\"\n",
    "\n",
    "**min_df** is used for removing terms that appear too infrequently\n",
    "min_df = 2 means \"ignore terms that appear in less than 2 complaints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQfvQA3Ae_El"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8fGwaCPJrGA"
   },
   "outputs": [],
   "source": [
    "#Write your code here to initialise the TfidfVectorizer \n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.95, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYzD85nTJrGA"
   },
   "source": [
    "#### Create a document term matrix using fit_transform\n",
    "\n",
    "The contents of a document term matrix are tuples of (complaint_id,token_id) tf-idf score:\n",
    "The tuples that are not there have a tf-idf score of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffzdDpp_JrGB"
   },
   "outputs": [],
   "source": [
    "#Write your code here to create the Document Term Matrix by transforming the complaints column present in df_clean.\n",
    "documet_term_matrix = tfidf.fit_transform(df_clean['Complaint_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyI1CGv-fdag",
    "outputId": "ad083ff0-8b1c-411d-97af-6473a7cef270"
   },
   "outputs": [],
   "source": [
    "documet_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q9lwvNEJrGB"
   },
   "source": [
    "## Topic Modelling using NMF\n",
    "\n",
    "Non-Negative Matrix Factorization (NMF) is an unsupervised technique so there are no labeling of topics that the model will be trained on. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative.\n",
    "\n",
    "In this task you have to perform the following:\n",
    "\n",
    "* Find the best number of clusters \n",
    "* Apply the best number to create word clusters\n",
    "* Inspect & validate the correction of each cluster wrt the complaints \n",
    "* Correct the labels if needed \n",
    "* Map the clusters to topics/cluster names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amLT4omWJrGB"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wYR1xUTJrGD"
   },
   "source": [
    "## Manual Topic Modeling\n",
    "You need to do take the trial & error approach to find the best num of topics for your NMF model.\n",
    "\n",
    "The only parameter that is required is the number of components i.e. the number of topics we want. This is the most crucial step in the whole topic modeling process and will greatly affect how good your final topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgd2A6bhJrGD"
   },
   "outputs": [],
   "source": [
    "#Load your nmf_model with the n_components i.e 5\n",
    "num_topics = 5 \n",
    "\n",
    "#keep the random_state =40\n",
    "nmf_model = NMF(n_components=num_topics, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPMDYbt_JrGE",
    "outputId": "220ac964-6cd0-4934-cdec-175908977320"
   },
   "outputs": [],
   "source": [
    "nmf_model.fit(documet_term_matrix)\n",
    "len(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "16kRfat5JrGE",
    "outputId": "9974df00-5cbd-4509-e3f5-7632309ab8c6"
   },
   "outputs": [],
   "source": [
    "#Print the Top15 words for each of the topics\n",
    "H = nmf_model.components_       # Topic-term matrix\n",
    "words = np.array(tfidf.get_feature_names_out())\n",
    "topic_words = pd.DataFrame(np.zeros((num_topics, 15)), index=[f'Topic {i + 1}' for i in range(num_topics)],\n",
    "                           columns=[f'Word {i + 1}' for i in range(15)]).astype(str)\n",
    "for i in range(num_topics):\n",
    "    ix = H[i].argsort()[::-1][:15]\n",
    "    topic_words.iloc[i] = words[ix]\n",
    "\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFGTkPAQhZlv"
   },
   "source": [
    "**Observation** Looking at the topics above, for each topic, we can give a label based on their products/services:\n",
    "\n",
    "Topic 1 = Bank account services\n",
    "\n",
    "Topic 2 = Credit card / Prepaid card\n",
    "\n",
    "Topic 3 = Others\n",
    "\n",
    "Topic 4 = Theft/Dispute reporting\n",
    "\n",
    "Topic 5 = Mortgages/loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OIT7LmFJrGF"
   },
   "outputs": [],
   "source": [
    "#Create the best topic for each complaint in terms of integer value 0,1,2,3 & 4\n",
    "topic_results = nmf_model.transform(documet_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peyYv-ORJrGF"
   },
   "outputs": [],
   "source": [
    "#Assign the best topic to each of the cmplaints in Topic Column\n",
    "df_clean['Topic'] =  topic_results.argmax(axis=1)#write your code to assign topics to each rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-l9zFpWZw91J",
    "outputId": "b177a084-67f8-4630-f3b7-b734eaa8fd50"
   },
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fLh_Gf3nJrGF",
    "outputId": "96a1b8af-2eb7-436d-dbf9-1bd4118ef47c"
   },
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "aQKpufSPJrGG",
    "outputId": "a020fc27-8d9d-440c-cc4c-2c4cee20d71e"
   },
   "outputs": [],
   "source": [
    "#Print the first 5 Complaint for each of the Topics\n",
    "df_clean_first_five=df_clean.groupby('Topic').head(5)\n",
    "df_clean_first_five.sort_values('Topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqRYHCOBxIKe",
    "outputId": "ba30a8cf-87c3-40f0-b540-c4920d99008e"
   },
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piyLxzj6v07j"
   },
   "source": [
    "#### After evaluating the mapping, if the topics assigned are correct then assign these names to the relevant topic:\n",
    "* Bank Account services\n",
    "* Credit card or prepaid card\n",
    "* Theft/Dispute Reporting\n",
    "* Mortgage/Loan\n",
    "* Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWpwDG4RJrGG"
   },
   "outputs": [],
   "source": [
    "#Create the dictionary of Topic names and Topics\n",
    "\n",
    "topic_names = { 0:\"Bank account services\", 1:\"Credit card / Prepaid card\", 2:\"Others\",3:\"Theft/Dispute reporting\", 4:\"Mortgages/loans\"   }\n",
    "#Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic'].map(topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2ULY5K6JrGG",
    "outputId": "c952bc39-0682-48ea-95dc-6ef0ba8669da"
   },
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ATHXwJ-4itQK",
    "outputId": "7d6067f4-0bd6-483c-e30e-b95d2549e109"
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Mu0QBOcJrGH"
   },
   "source": [
    "## Supervised model to predict any new complaints to the relevant Topics.\n",
    "\n",
    "You have now build the model to create the topics for each complaints.Now in the below section you will use them to classify any new complaints.\n",
    "\n",
    "Since you will be using supervised learning technique we have to convert the topic names to numbers(numpy arrays only understand numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_U8J3J8wJrGH"
   },
   "outputs": [],
   "source": [
    "#Create the dictionary again of Topic names and Topics\n",
    "\n",
    "topic_names = {  \"Bank account services\":0, \"Credit card / Prepaid card\":1, \"Others\":2, \"Theft/Dispute reporting\":3, \"Mortgages/loans\":4  }\n",
    "#Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic'].map(topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKIGX12fwYvJ",
    "outputId": "e21eec64-883e-4610-d05c-dc563d5bc4fe"
   },
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "BWIgJUkQJrGH",
    "outputId": "3bed878a-f1b2-4f49-fd92-7e8d19f5d6d9"
   },
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xx-FrbkWJrGH"
   },
   "outputs": [],
   "source": [
    "#Keep the columns\"complaint_what_happened\" & \"Topic\" only in the new dataframe --> training_data\n",
    "training_data=df_clean[['complaint_what_happened', 'Topic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVzTlkoYwSMJ",
    "outputId": "54bb1502-d4c9-4eae-e479-5723e6b0c0a9"
   },
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "lVg2pa12JrGI",
    "outputId": "5d5677bc-ce9b-4d15-e43d-faf1afd0c374"
   },
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "280Vbqk-7a8M"
   },
   "source": [
    "####Apply the supervised models on the training data created. In this process, you have to do the following:\n",
    "* Create the vector counts using Count Vectoriser\n",
    "* Transform the word vecotr to tf-idf\n",
    "* Create the train & test data using the train_test_split on the tf-idf & topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yu4f2LLqkfUf"
   },
   "outputs": [],
   "source": [
    "# Import pickle to save and load the model\n",
    "#import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUlQpgkzJrGI"
   },
   "outputs": [],
   "source": [
    "#Write your code to get the Vector count\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_cout = count_vectorizer.fit_transform(training_data['complaint_what_happened'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zytod0yOktFP"
   },
   "outputs": [],
   "source": [
    "#Write your code here to transform the word vector to tf-idf\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_cout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMU3vj6w-wqL"
   },
   "source": [
    "You have to try atleast 3 models on the train & test data from these options:\n",
    "* Logistic regression\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Naive Bayes (optional)\n",
    "\n",
    "**Using the required evaluation metrics judge the tried models and select the ones performing the best**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udLHpPsZJrGI"
   },
   "outputs": [],
   "source": [
    "# Write your code here to build any 3 models and evaluate them using the required metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeTjt4wAtg1M"
   },
   "source": [
    "# Logistic regression : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2OznsObJrGP"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCbq8CoAt0_3"
   },
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data['Topic'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nzJZEt1vtzv",
    "outputId": "16a5d41f-04b1-4d71-d8db-4c9afc6469d3"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FE1MkDXwvtuF",
    "outputId": "676e55ca-a1f5-428c-c775-f3ce25f24d57"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBVg1qJmuP3R",
    "outputId": "30f4203d-0172-40e2-adf3-29290b7b7a6c"
   },
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression().fit(X_train, y_train)\n",
    "predicted = logistic_regression.predict(X_test)\n",
    "\n",
    "topicnames_target = [\"Bank account services\", \"Credit card / Prepaid card\", \"Others\", \"Theft/Dispute reporting\", \"Mortgages/loans\"]\n",
    "print(classification_report(y_true=y_test, y_pred=predicted,target_names = topicnames_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPR7wyhGDJ-N"
   },
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fn_ZjZ2k-HQM"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPQtTphbvsnS",
    "outputId": "831d0fcf-5544-4523-d107-6156f0517d84"
   },
   "outputs": [],
   "source": [
    "# Decision tree classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "predicted = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred=predicted, y_true=y_test, target_names = topicnames_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fK9j_4aKE1oP"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdGlI0kx-5s5"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJAkJDLN_Yo8",
    "outputId": "eaf442df-0437-45e3-9a20-250c056eb534"
   },
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(max_depth=10)\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "predicted = random_forest_classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred=predicted, y_true=y_test,target_names = topicnames_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqjDH06WE9P2"
   },
   "source": [
    "# Naive Bayes (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQr8roI2_gs8"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSGHW_yJ_gEc",
    "outputId": "344c4d03-cc33-42cd-b9b8-b5a65a2a3872"
   },
   "outputs": [],
   "source": [
    "gaussian_navie_bayes = GaussianNB().fit(X_train.toarray(), y_train)\n",
    "predicted = gaussian_navie_bayes.predict(X_test.toarray())\n",
    "\n",
    "print(classification_report(y_pred=predicted, y_true=y_test,target_names = topicnames_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0j-fW46FP6L"
   },
   "source": [
    "# Observations :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsZL12kjFhQj"
   },
   "source": [
    "1. Logistic regression : Accuracy is 0.92  , f1-scores are 0.93, 0.93, 0.91 ,0.90 and 0.93 for classess \"Bank account services\", \"Credit card / Prepaid card\", \"Others\", \"Theft/Dispute reporting\", \"Mortgages/loans\" respectively.\n",
    "\n",
    "2. Decision Tree : Accuracy is 0.79  , f1-scores are 0.80, 0.81, 0.75, 0.74 and 0.82 for classess \"Bank account services\", \"Credit card / Prepaid card\", \"Others\", \"Theft/Dispute reporting\", \"Mortgages/loans\" respectively.\n",
    "\n",
    "3. Random Forest : Accuracy is 0.72  , f1-scores are 0.79, 0.73, 0.06, 0.73 and 0.79  for classess \"Bank account services\", \"Credit card / Prepaid card\", \"Others\", \"Theft/Dispute reporting\", \"Mortgages/loans\" respectively.\n",
    "\n",
    "4. Naive Bayes (optional) : Accuracy is 0.35  , f1-scores are 0.33,0.39, 0.23, 0.38 and 0.47 for classess \"Bank account services\", \"Credit card / Prepaid card\", \"Others\", \"Theft/Dispute reporting\", \"Mortgages/loans\" respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1GlKaaMIoS5"
   },
   "source": [
    "From above analysis , we can see that Logistic regression is giving best accuracy and f1-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gl9Nz_vwJNxF"
   },
   "source": [
    "# Using Logistic regression to predict ( some examples )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "TvbYt7pZB0iO",
    "outputId": "9c26ce34-672a-4403-beb6-718cd761dd60"
   },
   "outputs": [],
   "source": [
    "df_complaints = pd.DataFrame({'complaints': [\"I can not get from chase who services my mortgage, who owns it and who has original loan docs\", \n",
    "                                  \"The bill amount of my credit card was debited twice. Please look into the matter and resolve at the earliest.\",\n",
    "                                  \"I want to open a salary account at your downtown branch. Please provide me the procedure.\",\n",
    "                                  \"Yesterday, I received a fraudulent email regarding renewal of my services.\",\n",
    "                                  \"What is the procedure to know my CIBIL score?\",\n",
    "                                  \"I need to know the number of bank branches and their locations in the city of Dubai\"]})\n",
    "df_complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcHMNlqKB9Kw"
   },
   "outputs": [],
   "source": [
    "def predict_using_linear_regression(text):\n",
    "    topic_names = {0:'Account Services', 1:'Others', 2:'Mortgage/Loan', 3:'Credit card or prepaid card', 4:'Theft/Dispute Reporting'}\n",
    "    X_new_counts = count_vectorizer.transform(text)\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "    predicted = logistic_regression.predict(X_new_tfidf)\n",
    "    return topic_names[predicted[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "jUhA2NeiB-vb",
    "outputId": "58f15ca3-f091-442f-a8a2-fa1ab72c519e"
   },
   "outputs": [],
   "source": [
    "df_complaints['tag'] = df_complaints['complaints'].apply(lambda x: predict_using_linear_regression([x]))\n",
    "df_complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMAslLV-Ihfl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
